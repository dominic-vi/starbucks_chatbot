{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!curl https://ollama.ai/install.sh | sh\n\n!echo 'debconf debconf/frontend select Noninteractive' | sudo debconf-set-selections\n!sudo apt-get update && sudo apt-get install -y cuda-drivers\n\n!pip install pyngrok\nfrom pyngrok import ngrok\nngrok.set_auth_token('2nyf6MOWofrvsOrHFEZV5PMd5LX_4k2TGAy2uPFDqUVpyiey7')\n\nimport os\nimport asyncio\n\nos.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\nasync def run_process(cmd):\n  print('>>> starting', *cmd)\n  p = await asyncio.subprocess.create_subprocess_exec(\n      *cmd,\n      stdout=asyncio.subprocess.PIPE,\n      stderr=asyncio.subprocess.PIPE,\n  )\n  async def pipe(lines):\n    async for line in lines:\n      print(line.strip().decode('utf-8'))\n  await asyncio.gather(\n      pipe(p.stdout),\n      pipe(p.stderr),\n  )\nfrom IPython.display import clear_output\nclear_output()\n\nawait asyncio.gather(\nrun_process(['ollama', 'serve']),\nrun_process(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header=\"localhost:11434\"']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T09:38:34.928398Z","iopub.execute_input":"2024-12-19T09:38:34.929015Z"}},"outputs":[{"name":"stdout","text":">>> starting ollama serve\n>>> starting ngrok http --log stderr 11434 --host-header=\"localhost:11434\"\nCouldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is:\n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIBgHDtP3MPeCRFEbbjTn2DFCJ7R03h+4dMMqMsqoFbK0\n\n2024/12/19 09:40:19 routes.go:1259: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2024-12-19T09:40:19.518Z level=INFO source=images.go:757 msg=\"total blobs: 0\"\ntime=2024-12-19T09:40:19.518Z level=INFO source=images.go:764 msg=\"total unused blobs removed: 0\"\n[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n\n[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n- using env:\texport GIN_MODE=release\n- using code:\tgin.SetMode(gin.ReleaseMode)\n\n[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\n[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\ntime=2024-12-19T09:40:19.519Z level=INFO source=routes.go:1310 msg=\"Listening on 127.0.0.1:11434 (version 0.5.4)\"\ntime=2024-12-19T09:40:19.519Z level=INFO source=routes.go:1339 msg=\"Dynamic LLM libraries\" runners=\"[cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx rocm_avx]\"\ntime=2024-12-19T09:40:19.519Z level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\nt=2024-12-19T09:40:19+0000 lvl=info msg=\"no configuration paths supplied\"\nt=2024-12-19T09:40:19+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\nt=2024-12-19T09:40:19+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\ntime=2024-12-19T09:40:19.659Z level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-7c01a246-4b6c-3377-6c47-8ad7004719ab library=cuda variant=v12 compute=6.0 driver=12.7 name=\"Tesla P100-PCIE-16GB\" total=\"15.9 GiB\" available=\"15.6 GiB\"\nt=2024-12-19T09:40:19+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\nt=2024-12-19T09:40:20+0000 lvl=info msg=\"client session established\" obj=tunnels.session\nt=2024-12-19T09:40:20+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\nt=2024-12-19T09:40:20+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://b64c-34-34-81-230.ngrok-free.app\nt=2024-12-19T09:40:27+0000 lvl=info msg=\"join connections\" obj=join id=c936fb699a3d l=127.0.0.1:11434 r=116.110.43.252:49060\n[GIN] 2024/12/19 - 09:40:27 | 200 |      93.161µs |  116.110.43.252 | GET      \"/\"\n[GIN] 2024/12/19 - 09:40:28 | 404 |        7.11µs |  116.110.43.252 | GET      \"/favicon.ico\"\nt=2024-12-19T09:41:12+0000 lvl=info msg=\"join connections\" obj=join id=5ddd64e0fbf4 l=127.0.0.1:11434 r=116.110.43.252:28912\n[GIN] 2024/12/19 - 09:41:12 | 200 |      42.776µs |  116.110.43.252 | HEAD     \"/\"\n[GIN] 2024/12/19 - 09:41:12 | 200 |     560.641µs |  116.110.43.252 | GET      \"/api/tags\"\nt=2024-12-19T09:41:24+0000 lvl=info msg=\"join connections\" obj=join id=1bb3bb65f0a6 l=127.0.0.1:11434 r=116.110.43.252:48683\n[GIN] 2024/12/19 - 09:41:24 | 200 |      41.515µs |  116.110.43.252 | HEAD     \"/\"\ntime=2024-12-19T09:41:26.045Z level=INFO source=download.go:175 msg=\"downloading 43e45eda3e40 in 16 323 MB part(s)\"\ntime=2024-12-19T09:41:42.389Z level=INFO source=download.go:175 msg=\"downloading 63484f08b0d2 in 1 933 B part(s)\"\ntime=2024-12-19T09:41:43.713Z level=INFO source=download.go:175 msg=\"downloading f02dd72bb242 in 1 59 B part(s)\"\ntime=2024-12-19T09:41:45.035Z level=INFO source=download.go:175 msg=\"downloading 754d5b38a0a1 in 1 413 B part(s)\"\n[GIN] 2024/12/19 - 09:42:01 | 200 | 36.554600145s |  116.110.43.252 | POST     \"/api/pull\"\nt=2024-12-19T09:43:12+0000 lvl=info msg=\"join connections\" obj=join id=4788185c5781 l=127.0.0.1:11434 r=116.110.43.252:49813\ntime=2024-12-19T09:43:12.600Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\ntime=2024-12-19T09:43:12.725Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 gpu=GPU-7c01a246-4b6c-3377-6c47-8ad7004719ab parallel=4 available=16790978560 required=\"6.9 GiB\"\ntime=2024-12-19T09:43:12.824Z level=INFO source=server.go:104 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.9 GiB\" free_swap=\"0 B\"\ntime=2024-12-19T09:43:12.825Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.9 GiB\" memory.required.partial=\"6.9 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.9 GiB]\" memory.weights.total=\"5.6 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"123.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"585.0 MiB\"\ntime=2024-12-19T09:43:12.826Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 2 --parallel 4 --port 45563\"\ntime=2024-12-19T09:43:12.826Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2024-12-19T09:43:12.826Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\ntime=2024-12-19T09:43:12.827Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2024-12-19T09:43:12.888Z level=INFO source=runner.go:945 msg=\"starting go runner\"\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\ntime=2024-12-19T09:43:12.899Z level=INFO source=runner.go:946 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=2\ntime=2024-12-19T09:43:12.899Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:45563\"\nllama_load_model_from_file: using device CUDA0 (Tesla P100-PCIE-16GB) - 16013 MiB free\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = .\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,38369]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,38369]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,38369]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,69853]   = [\"▁b ọt\", \"▁bọ t\", \"▁V ẤN...\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\ntime=2024-12-19T09:43:13.078Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 7\nllm_load_vocab: token to piece cache size = 0.1979 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 38369\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 7.29 B\nllm_load_print_meta: model size       = 4.81 GiB (5.67 BPW)\nllm_load_print_meta: general.name     = .\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size =   103.04 MiB\nllm_load_tensors:        CUDA0 model buffer size =  4827.46 MiB\nllama_new_context_with_model: n_seq_max     = 4\nllama_new_context_with_model: n_ctx         = 8192\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 2048\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 10000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.65 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\ntime=2024-12-19T09:43:15.339Z level=INFO source=server.go:594 msg=\"llama runner started in 2.51 seconds\"\n[GIN] 2024/12/19 - 09:43:17 | 200 |  4.564447546s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:43:34+0000 lvl=info msg=\"join connections\" obj=join id=b2f2958b337a l=127.0.0.1:11434 r=116.110.43.252:6574\ntime=2024-12-19T09:43:34.477Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:43:39 | 200 |  4.795376238s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:44:00+0000 lvl=info msg=\"join connections\" obj=join id=79d328930dd1 l=127.0.0.1:11434 r=116.110.43.252:54334\ntime=2024-12-19T09:44:00.365Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:44:02 | 200 |  2.230527727s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:52:52+0000 lvl=info msg=\"join connections\" obj=join id=c94ab8337d22 l=127.0.0.1:11434 r=116.110.43.252:17345\ntime=2024-12-19T09:52:52.390Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\ntime=2024-12-19T09:52:52.522Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 gpu=GPU-7c01a246-4b6c-3377-6c47-8ad7004719ab parallel=4 available=16790978560 required=\"6.9 GiB\"\ntime=2024-12-19T09:52:52.625Z level=INFO source=server.go:104 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.9 GiB\" free_swap=\"0 B\"\ntime=2024-12-19T09:52:52.627Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.9 GiB\" memory.required.partial=\"6.9 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.9 GiB]\" memory.weights.total=\"5.6 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"123.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"585.0 MiB\"\ntime=2024-12-19T09:52:52.628Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 2 --parallel 4 --port 34237\"\ntime=2024-12-19T09:52:52.628Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2024-12-19T09:52:52.628Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\ntime=2024-12-19T09:52:52.629Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2024-12-19T09:52:52.691Z level=INFO source=runner.go:945 msg=\"starting go runner\"\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\ntime=2024-12-19T09:52:52.700Z level=INFO source=runner.go:946 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=2\ntime=2024-12-19T09:52:52.700Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:34237\"\nllama_load_model_from_file: using device CUDA0 (Tesla P100-PCIE-16GB) - 16013 MiB free\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = .\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,38369]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,38369]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,38369]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,69853]   = [\"▁b ọt\", \"▁bọ t\", \"▁V ẤN...\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 7\ntime=2024-12-19T09:52:52.880Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: token to piece cache size = 0.1979 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 38369\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 7.29 B\nllm_load_print_meta: model size       = 4.81 GiB (5.67 BPW)\nllm_load_print_meta: general.name     = .\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size =   103.04 MiB\nllm_load_tensors:        CUDA0 model buffer size =  4827.46 MiB\nllama_new_context_with_model: n_seq_max     = 4\nllama_new_context_with_model: n_ctx         = 8192\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 2048\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 10000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.65 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\ntime=2024-12-19T09:52:54.639Z level=INFO source=server.go:594 msg=\"llama runner started in 2.01 seconds\"\n[GIN] 2024/12/19 - 09:52:56 | 200 |   4.05466917s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:53:18+0000 lvl=info msg=\"join connections\" obj=join id=991b76557f20 l=127.0.0.1:11434 r=116.110.43.252:35600\ntime=2024-12-19T09:53:18.705Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:53:22 | 200 |  3.818463099s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:57:02+0000 lvl=info msg=\"join connections\" obj=join id=e52d680b2d6a l=127.0.0.1:11434 r=116.110.43.252:49746\ntime=2024-12-19T09:57:02.356Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:57:06 | 200 |  3.902085882s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:57:35+0000 lvl=info msg=\"join connections\" obj=join id=567875dcbe82 l=127.0.0.1:11434 r=116.110.43.252:10712\ntime=2024-12-19T09:57:35.102Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:57:39 | 200 |  4.269703568s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:57:59+0000 lvl=info msg=\"join connections\" obj=join id=024a46a9fccf l=127.0.0.1:11434 r=116.110.43.252:42917\ntime=2024-12-19T09:57:59.617Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:58:00 | 200 |  830.806634ms |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:58:33+0000 lvl=info msg=\"join connections\" obj=join id=cf8936620594 l=127.0.0.1:11434 r=116.110.43.252:48515\ntime=2024-12-19T09:58:33.241Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:58:34 | 200 |   1.32953932s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:58:44+0000 lvl=info msg=\"join connections\" obj=join id=10a6c1d94a7c l=127.0.0.1:11434 r=116.110.43.252:47637\ntime=2024-12-19T09:58:44.703Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:58:46 | 200 |  2.005845403s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:59:04+0000 lvl=info msg=\"join connections\" obj=join id=e4d208ec19c3 l=127.0.0.1:11434 r=116.110.43.252:6766\ntime=2024-12-19T09:59:04.632Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:59:06 | 200 |  2.029935914s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:59:27+0000 lvl=info msg=\"join connections\" obj=join id=35d9d3d01b23 l=127.0.0.1:11434 r=116.110.43.252:5930\ntime=2024-12-19T09:59:27.688Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:59:31 | 200 |  3.587643347s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T09:59:49+0000 lvl=info msg=\"join connections\" obj=join id=a90b7e5ee66c l=127.0.0.1:11434 r=116.110.43.252:63471\ntime=2024-12-19T09:59:49.983Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 09:59:51 | 200 |  1.921479992s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T10:00:10+0000 lvl=info msg=\"join connections\" obj=join id=25149f06cb21 l=127.0.0.1:11434 r=116.110.43.252:59131\ntime=2024-12-19T10:00:10.480Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 10:00:12 | 200 |  2.366655997s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T10:00:34+0000 lvl=info msg=\"join connections\" obj=join id=6130ae433831 l=127.0.0.1:11434 r=116.110.43.252:42519\ntime=2024-12-19T10:00:34.418Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 10:00:37 | 200 |  2.857543233s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T10:10:36+0000 lvl=info msg=\"join connections\" obj=join id=6672be65c1b0 l=127.0.0.1:11434 r=116.110.43.252:62934\ntime=2024-12-19T10:10:36.534Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\ntime=2024-12-19T10:10:36.663Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 gpu=GPU-7c01a246-4b6c-3377-6c47-8ad7004719ab parallel=4 available=16790978560 required=\"6.9 GiB\"\ntime=2024-12-19T10:10:36.767Z level=INFO source=server.go:104 msg=\"system memory\" total=\"31.4 GiB\" free=\"29.9 GiB\" free_swap=\"0 B\"\ntime=2024-12-19T10:10:36.768Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[15.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.9 GiB\" memory.required.partial=\"6.9 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.9 GiB]\" memory.weights.total=\"5.6 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"123.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"585.0 MiB\"\ntime=2024-12-19T10:10:36.768Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 2 --parallel 4 --port 46433\"\ntime=2024-12-19T10:10:36.769Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\ntime=2024-12-19T10:10:36.769Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\ntime=2024-12-19T10:10:36.771Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\ntime=2024-12-19T10:10:36.834Z level=INFO source=runner.go:945 msg=\"starting go runner\"\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\nDevice 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\ntime=2024-12-19T10:10:36.843Z level=INFO source=runner.go:946 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=2\ntime=2024-12-19T10:10:36.843Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:46433\"\nllama_load_model_from_file: using device CUDA0 (Tesla P100-PCIE-16GB) - 16013 MiB free\nllama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /root/.ollama/models/blobs/sha256-43e45eda3e401accacbb904c9d05fb30ef024011e1e98007d1d7449f301a6f50 (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = .\nllama_model_loader: - kv   2:                       llama.context_length u32              = 32768\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\nllama_model_loader: - kv  11:                          general.file_type u32              = 17\nllama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,38369]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,38369]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,38369]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,69853]   = [\"▁b ọt\", \"▁bọ t\", \"▁V ẤN...\nllama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 0\nllama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if messages[0]['role'] == 'system'...\nllama_model_loader: - kv  22:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q5_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\ntime=2024-12-19T10:10:37.022Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\nllm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nllm_load_vocab: special tokens cache size = 7\nllm_load_vocab: token to piece cache size = 0.1979 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = SPM\nllm_load_print_meta: n_vocab          = 38369\nllm_load_print_meta: n_merges         = 0\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 32768\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 10000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 32768\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: ssm_dt_b_c_rms   = 0\nllm_load_print_meta: model type       = 7B\nllm_load_print_meta: model ftype      = Q5_K - Medium\nllm_load_print_meta: model params     = 7.29 B\nllm_load_print_meta: model size       = 4.81 GiB (5.67 BPW)\nllm_load_print_meta: general.name     = .\nllm_load_print_meta: BOS token        = 1 '<s>'\nllm_load_print_meta: EOS token        = 2 '</s>'\nllm_load_print_meta: UNK token        = 0 '<unk>'\nllm_load_print_meta: PAD token        = 0 '<unk>'\nllm_load_print_meta: LF token         = 13 '<0x0A>'\nllm_load_print_meta: EOG token        = 2 '</s>'\nllm_load_print_meta: max token length = 48\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading output layer to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:   CPU_Mapped model buffer size =   103.04 MiB\nllm_load_tensors:        CUDA0 model buffer size =  4827.46 MiB\nllama_new_context_with_model: n_seq_max     = 4\nllama_new_context_with_model: n_ctx         = 8192\nllama_new_context_with_model: n_ctx_per_seq = 2048\nllama_new_context_with_model: n_batch       = 2048\nllama_new_context_with_model: n_ubatch      = 512\nllama_new_context_with_model: flash_attn    = 0\nllama_new_context_with_model: freq_base     = 10000.0\nllama_new_context_with_model: freq_scale    = 1\nllama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\nllama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\nllama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.65 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\ntime=2024-12-19T10:10:38.780Z level=INFO source=server.go:594 msg=\"llama runner started in 2.01 seconds\"\n[GIN] 2024/12/19 - 10:10:42 | 200 |  6.325803803s |  116.110.43.252 | POST     \"/api/chat\"\nt=2024-12-19T10:10:58+0000 lvl=info msg=\"join connections\" obj=join id=9759eaf0b4a3 l=127.0.0.1:11434 r=116.110.43.252:46226\ntime=2024-12-19T10:10:58.704Z level=WARN source=types.go:509 msg=\"invalid option provided\" option=tfs_z\n[GIN] 2024/12/19 - 10:11:02 | 200 |  3.946740766s |  116.110.43.252 | POST     \"/api/chat\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 22\u001b[0m, in \u001b[0;36mrun_process.<locals>.pipe\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpipe\u001b[39m(lines):\n\u001b[0;32m---> 22\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:723\u001b[0m, in \u001b[0;36mStreamReader.__anext__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 723\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreadline()\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:524\u001b[0m, in \u001b[0;36mStreamReader.readline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaduntil(sep)\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mIncompleteReadError \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:616\u001b[0m, in \u001b[0;36mStreamReader.readuntil\u001b[0;34m(self, separator)\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;66;03m# _wait_for_data() will resume reading if stream was paused.\u001b[39;00m\n\u001b[0;32m--> 616\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreaduntil\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isep \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit:\n","File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/streams.py:501\u001b[0m, in \u001b[0;36mStreamReader._wait_for_data\u001b[0;34m(self, func_name)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_waiter\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mCancelledError\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clear_output\n\u001b[1;32m     29\u001b[0m clear_output()\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\n\u001b[1;32m     32\u001b[0m run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mollama\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserve\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     33\u001b[0m run_process([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mngrok\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--log\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstderr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m11434\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--host-header=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocalhost:11434\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n","\u001b[0;31mCancelledError\u001b[0m: "],"ename":"CancelledError","evalue":"","output_type":"error"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}